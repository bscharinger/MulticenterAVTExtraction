{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernel_info": {
   "name": "azureml_py38_tensorflow"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "colab": {
   "name": "main (1).ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "gather": {
     "logged": 1635980540292
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "id": "vo77MIgCU3bW"
   },
   "source": [
    "from __future__ import division\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import nrrd\n",
    "import numpy as np\n",
    "import time\n",
    "from data_gen import CTA\n",
    "from tensorflow.python.client import device_lib\n",
    "from Networks import model_unet16, model_unet16csam, model_unet176, model_unet176csam, model_unet352\n",
    "import skimage\n",
    "import patchify\n",
    "from helper_funcs import *"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Config paths and mixed precision for reduced memory\n",
    "Load model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ],
   "metadata": {
    "id": "0zcH4AMACF9-"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1766: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2070, compute capability 7.5\n",
      "(None, 256, 256, 128, 1)\n",
      "Model: \"model\"\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "input_1 (InputLayer)                             [(None, 256, 256, 128, 1)]       0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d (Conv3D)                                  (None, 256, 256, 128, 8)         224               input_1[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization (BatchNormalization)         (None, 256, 256, 128, 8)         32                conv3d[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation (Activation)                          (None, 256, 256, 128, 8)         0                 batch_normalization[0][0]                         \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)                                (None, 256, 256, 128, 8)         1736              activation[0][0]                                  \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNormalization)       (None, 256, 256, 128, 8)         32                conv3d_1[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_1 (Activation)                        (None, 256, 256, 128, 8)         0                 batch_normalization_1[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D)                     (None, 128, 128, 64, 8)          0                 activation_1[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)                                (None, 128, 128, 64, 16)         3472              max_pooling3d[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNormalization)       (None, 128, 128, 64, 16)         64                conv3d_2[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_2 (Activation)                        (None, 128, 128, 64, 16)         0                 batch_normalization_2[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)                                (None, 128, 128, 64, 16)         6928              activation_2[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNormalization)       (None, 128, 128, 64, 16)         64                conv3d_3[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_3 (Activation)                        (None, 128, 128, 64, 16)         0                 batch_normalization_3[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)                   (None, 64, 64, 32, 16)           0                 activation_3[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)                                (None, 64, 64, 32, 32)           13856             max_pooling3d_1[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNormalization)       (None, 64, 64, 32, 32)           128               conv3d_4[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_4 (Activation)                        (None, 64, 64, 32, 32)           0                 batch_normalization_4[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)                                (None, 64, 64, 32, 32)           27680             activation_4[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNormalization)       (None, 64, 64, 32, 32)           128               conv3d_5[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_5 (Activation)                        (None, 64, 64, 32, 32)           0                 batch_normalization_5[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)                   (None, 32, 32, 16, 32)           0                 activation_5[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)                                (None, 32, 32, 16, 64)           55360             max_pooling3d_2[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNormalization)       (None, 32, 32, 16, 64)           256               conv3d_6[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_6 (Activation)                        (None, 32, 32, 16, 64)           0                 batch_normalization_6[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)                                (None, 32, 32, 16, 64)           110656            activation_6[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNormalization)       (None, 32, 32, 16, 64)           256               conv3d_7[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_7 (Activation)                        (None, 32, 32, 16, 64)           0                 batch_normalization_7[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "up_sampling3d (UpSampling3D)                     (None, 64, 64, 32, 64)           0                 activation_7[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate (Concatenate)                        (None, 64, 64, 32, 96)           0                 up_sampling3d[0][0]                               \n",
      "                                                                                                    activation_5[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)                                (None, 64, 64, 32, 32)           82976             concatenate[0][0]                                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNormalization)       (None, 64, 64, 32, 32)           128               conv3d_8[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_8 (Activation)                        (None, 64, 64, 32, 32)           0                 batch_normalization_8[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)                                (None, 64, 64, 32, 32)           27680             activation_8[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNormalization)       (None, 64, 64, 32, 32)           128               conv3d_9[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_9 (Activation)                        (None, 64, 64, 32, 32)           0                 batch_normalization_9[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)                   (None, 128, 128, 64, 32)         0                 activation_9[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)                      (None, 128, 128, 64, 48)         0                 up_sampling3d_1[0][0]                             \n",
      "                                                                                                    activation_3[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)                               (None, 128, 128, 64, 16)         20752             concatenate_1[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNormalization)      (None, 128, 128, 64, 16)         64                conv3d_10[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_10 (Activation)                       (None, 128, 128, 64, 16)         0                 batch_normalization_10[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)                               (None, 128, 128, 64, 16)         6928              activation_10[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNormalization)      (None, 128, 128, 64, 16)         64                conv3d_11[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_11 (Activation)                       (None, 128, 128, 64, 16)         0                 batch_normalization_11[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "up_sampling3d_2 (UpSampling3D)                   (None, 256, 256, 128, 16)        0                 activation_11[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)                      (None, 256, 256, 128, 24)        0                 up_sampling3d_2[0][0]                             \n",
      "                                                                                                    activation_1[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)                               (None, 256, 256, 128, 8)         5192              concatenate_2[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNormalization)      (None, 256, 256, 128, 8)         32                conv3d_12[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_12 (Activation)                       (None, 256, 256, 128, 8)         0                 batch_normalization_12[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)                               (None, 256, 256, 128, 8)         1736              activation_12[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNormalization)      (None, 256, 256, 128, 8)         32                conv3d_13[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_13 (Activation)                       (None, 256, 256, 128, 8)         0                 batch_normalization_13[0][0]                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)                               (None, 256, 256, 128, 1)         9                 activation_13[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_14 (Activation)                       (None, 256, 256, 128, 1)         0                 conv3d_14[0][0]                                   \n",
      "======================================================================================================================================================\n",
      "Total params: 366,593\n",
      "Trainable params: 365,889\n",
      "Non-trainable params: 704\n",
      "______________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "training_dir = \"./training_data/train_z128_iso_win_im/\"\n",
    "label_dir = \"./training_data/train_z128_iso_win_la/\"\n",
    "data_list =glob('{}/*.nrrd'.format(training_dir))\n",
    "label_list=glob('{}/*.nrrd'.format(label_dir))\n",
    "\n",
    "policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "img_size = (256, 256, 128)\n",
    "model = model_classic_unet_176.get_model(img_size)\n",
    "a=model.summary(line_length=150)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split data into training and validation set\n",
    "Initialise sequence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "val_samples = int(np.floor(len(data_list)*0.3))\n",
    "random.Random(7331).shuffle(data_list)\n",
    "random.Random(7331).shuffle(label_list)\n",
    "\n",
    "train_img_paths = data_list[:-val_samples]\n",
    "train_lab_paths = label_list[:-val_samples]\n",
    "\n",
    "val_img_paths = data_list[-val_samples:]\n",
    "val_lab_paths = label_list[-val_samples:]\n",
    "\n",
    "train_gen = CTA(train_img_paths, train_lab_paths, 1)\n",
    "valid_gen = CTA(val_img_paths, val_lab_paths, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Enable dynamic memory allocation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compile model\n",
    "Add callback to check loss inside of epoch\n",
    "train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxFe6hDqU3bc",
    "outputId": "376caa2e-5f02-4a0f-dc64-f01c4ba267f8"
   },
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001, epsilon=0.01), loss=[neg_dice_coef])\n",
    "epochs = 300\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=20, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00000001, verbose=1),\n",
    "    keras.callbacks.ModelCheckpoint(\"classic_unet_176.h5\", verbose=1, save_best_only=True)\n",
    "]\n",
    "start = time.time()\n",
    "history = model.fit(train_gen, epochs=epochs, validation_data=valid_gen, callbacks=callbacks,shuffle=True)\n",
    "end = time.time()\n",
    "print('Training time: ', end-start)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "175/175 [==============================] - 225s 1s/step - loss: -88666.8438 - val_loss: -89421.3125\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -89421.31250, saving model to classic_unet_176.h5\n",
      "Epoch 2/300\n",
      "175/175 [==============================] - 184s 1s/step - loss: -96486.1406 - val_loss: -96276.9297\n",
      "\n",
      "Epoch 00002: val_loss improved from -89421.31250 to -96276.92969, saving model to classic_unet_176.h5\n",
      "Epoch 3/300\n",
      "175/175 [==============================] - 187s 1s/step - loss: -98369.5391 - val_loss: -98490.2422\n",
      "\n",
      "Epoch 00003: val_loss improved from -96276.92969 to -98490.24219, saving model to classic_unet_176.h5\n",
      "Epoch 4/300\n",
      "175/175 [==============================] - 185s 1s/step - loss: -99015.3828 - val_loss: -99095.6797\n",
      "\n",
      "Epoch 00004: val_loss improved from -98490.24219 to -99095.67969, saving model to classic_unet_176.h5\n",
      "Epoch 5/300\n",
      "175/175 [==============================] - 184s 1s/step - loss: -99299.3828 - val_loss: -99087.4688\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -99095.67969\n",
      "Epoch 6/300\n",
      "175/175 [==============================] - 171s 977ms/step - loss: -99437.4844 - val_loss: -99467.1328\n",
      "\n",
      "Epoch 00006: val_loss improved from -99095.67969 to -99467.13281, saving model to classic_unet_176.h5\n",
      "Epoch 7/300\n",
      "175/175 [==============================] - 170s 970ms/step - loss: -99512.2266 - val_loss: -99357.1953\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -99467.13281\n",
      "Epoch 8/300\n",
      "175/175 [==============================] - 175s 998ms/step - loss: -99586.8672 - val_loss: -99516.1172\n",
      "\n",
      "Epoch 00008: val_loss improved from -99467.13281 to -99516.11719, saving model to classic_unet_176.h5\n",
      "Epoch 9/300\n",
      "175/175 [==============================] - 168s 960ms/step - loss: -99631.6953 - val_loss: -99644.6250\n",
      "\n",
      "Epoch 00009: val_loss improved from -99516.11719 to -99644.62500, saving model to classic_unet_176.h5\n",
      "Epoch 10/300\n",
      "175/175 [==============================] - 168s 958ms/step - loss: -99659.3750 - val_loss: -99612.7812\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -99644.62500\n",
      "Epoch 11/300\n",
      "175/175 [==============================] - 173s 988ms/step - loss: -99679.7734 - val_loss: -99496.3047\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -99644.62500\n",
      "Epoch 12/300\n",
      "175/175 [==============================] - 177s 1s/step - loss: -99708.4219 - val_loss: -99707.2578\n",
      "\n",
      "Epoch 00012: val_loss improved from -99644.62500 to -99707.25781, saving model to classic_unet_176.h5\n",
      "Epoch 13/300\n",
      "175/175 [==============================] - 182s 1s/step - loss: -99732.8125 - val_loss: -99634.2031\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -99707.25781\n",
      "Epoch 14/300\n",
      "175/175 [==============================] - 184s 1s/step - loss: -99752.5859 - val_loss: -99723.7969\n",
      "\n",
      "Epoch 00014: val_loss improved from -99707.25781 to -99723.79688, saving model to classic_unet_176.h5\n",
      "Epoch 15/300\n",
      "175/175 [==============================] - 187s 1s/step - loss: -99760.6875 - val_loss: -99745.1406\n",
      "\n",
      "Epoch 00015: val_loss improved from -99723.79688 to -99745.14062, saving model to classic_unet_176.h5\n",
      "Epoch 16/300\n",
      "175/175 [==============================] - 201s 1s/step - loss: -99772.3750 - val_loss: -99761.3750\n",
      "\n",
      "Epoch 00016: val_loss improved from -99745.14062 to -99761.37500, saving model to classic_unet_176.h5\n",
      "Epoch 17/300\n",
      "175/175 [==============================] - 190s 1s/step - loss: -99781.8984 - val_loss: -99610.6016\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -99761.37500\n",
      "Epoch 18/300\n",
      "175/175 [==============================] - 204s 1s/step - loss: -99805.9453 - val_loss: -99776.8516\n",
      "\n",
      "Epoch 00018: val_loss improved from -99761.37500 to -99776.85156, saving model to classic_unet_176.h5\n",
      "Epoch 19/300\n",
      "175/175 [==============================] - 189s 1s/step - loss: -99805.4062 - val_loss: -99741.1172\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -99776.85156\n",
      "Epoch 20/300\n",
      "175/175 [==============================] - 177s 1s/step - loss: -99818.3750 - val_loss: -99760.1719\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -99776.85156\n",
      "Epoch 21/300\n",
      "175/175 [==============================] - 213s 1s/step - loss: -99825.7344 - val_loss: -99804.3750\n",
      "\n",
      "Epoch 00021: val_loss improved from -99776.85156 to -99804.37500, saving model to classic_unet_176.h5\n",
      "Epoch 22/300\n",
      "175/175 [==============================] - 213s 1s/step - loss: -99839.2812 - val_loss: -99813.4766\n",
      "\n",
      "Epoch 00022: val_loss improved from -99804.37500 to -99813.47656, saving model to classic_unet_176.h5\n",
      "Epoch 23/300\n",
      "175/175 [==============================] - 180s 1s/step - loss: -99840.2031 - val_loss: -99803.3438\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -99813.47656\n",
      "Epoch 24/300\n",
      "175/175 [==============================] - 183s 1s/step - loss: -99847.2109 - val_loss: -99541.3125\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -99813.47656\n",
      "Epoch 25/300\n",
      "175/175 [==============================] - 176s 1s/step - loss: -99857.7031 - val_loss: -99556.8359\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -99813.47656\n",
      "Epoch 26/300\n",
      "175/175 [==============================] - 175s 999ms/step - loss: -99860.1484 - val_loss: -99769.4219\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -99813.47656\n",
      "Epoch 27/300\n",
      "175/175 [==============================] - 176s 1s/step - loss: -99866.5703 - val_loss: -99475.5000\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -99813.47656\n",
      "Epoch 28/300\n",
      "175/175 [==============================] - 176s 1s/step - loss: -99865.2109 - val_loss: -99804.8359\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -99813.47656\n",
      "Epoch 29/300\n",
      "175/175 [==============================] - 176s 1s/step - loss: -99865.2031 - val_loss: -99760.0391\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -99813.47656\n",
      "Epoch 30/300\n",
      "175/175 [==============================] - 179s 1s/step - loss: -99878.8594 - val_loss: -99694.3203\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -99813.47656\n",
      "Epoch 31/300\n",
      "175/175 [==============================] - 178s 1s/step - loss: -99883.8047 - val_loss: -99795.3438\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -99813.47656\n",
      "Epoch 32/300\n",
      "175/175 [==============================] - 176s 1s/step - loss: -99887.9219 - val_loss: -99801.6250\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -99813.47656\n",
      "Epoch 33/300\n",
      "175/175 [==============================] - 178s 1s/step - loss: -99900.2656 - val_loss: -99818.6797\n",
      "\n",
      "Epoch 00033: val_loss improved from -99813.47656 to -99818.67969, saving model to classic_unet_176.h5\n",
      "Epoch 34/300\n",
      "175/175 [==============================] - 178s 1s/step - loss: -99903.2031 - val_loss: -99822.7812\n",
      "\n",
      "Epoch 00034: val_loss improved from -99818.67969 to -99822.78125, saving model to classic_unet_176.h5\n",
      "Epoch 35/300\n",
      "175/175 [==============================] - 181s 1s/step - loss: -99905.2031 - val_loss: -99804.8906\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -99822.78125\n",
      "Epoch 36/300\n",
      "175/175 [==============================] - 172s 983ms/step - loss: -99906.5625 - val_loss: -99812.0234\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -99822.78125\n",
      "Epoch 37/300\n",
      "175/175 [==============================] - 176s 1s/step - loss: -99907.9297 - val_loss: -99817.6016\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -99822.78125\n",
      "Epoch 38/300\n",
      "175/175 [==============================] - 177s 1s/step - loss: -99908.9453 - val_loss: -99804.4453\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -99822.78125\n",
      "Epoch 39/300\n",
      "175/175 [==============================] - 186s 1s/step - loss: -99910.6094 - val_loss: -99809.5547\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -99822.78125\n",
      "Epoch 40/300\n",
      "175/175 [==============================] - 219s 1s/step - loss: -99911.9766 - val_loss: -99791.9766\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -99822.78125\n",
      "Epoch 41/300\n",
      "175/175 [==============================] - 217s 1s/step - loss: -99913.2812 - val_loss: -99828.7812\n",
      "\n",
      "Epoch 00041: val_loss improved from -99822.78125 to -99828.78125, saving model to classic_unet_176.h5\n",
      "Epoch 42/300\n",
      "175/175 [==============================] - 216s 1s/step - loss: -99914.2734 - val_loss: -99812.3438\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -99828.78125\n",
      "Epoch 43/300\n",
      "175/175 [==============================] - 179s 1s/step - loss: -99915.6250 - val_loss: -99803.6562\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -99828.78125\n",
      "Epoch 44/300\n",
      "175/175 [==============================] - 172s 981ms/step - loss: -99916.7578 - val_loss: -99782.2891\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -99828.78125\n",
      "Epoch 45/300\n",
      "175/175 [==============================] - 170s 969ms/step - loss: -99917.7578 - val_loss: -99807.2969\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -99828.78125\n",
      "Epoch 46/300\n",
      "175/175 [==============================] - 167s 954ms/step - loss: -99918.9609 - val_loss: -99829.1406\n",
      "\n",
      "Epoch 00046: val_loss improved from -99828.78125 to -99829.14062, saving model to classic_unet_176.h5\n",
      "Epoch 47/300\n",
      "175/175 [==============================] - 167s 950ms/step - loss: -99919.9922 - val_loss: -99810.1797\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -99829.14062\n",
      "Epoch 48/300\n",
      "175/175 [==============================] - 167s 953ms/step - loss: -99921.0312 - val_loss: -99793.4609\n",
      "\n",
      "Epoch 00048: val_loss did not improve from -99829.14062\n",
      "Epoch 49/300\n",
      "175/175 [==============================] - 167s 955ms/step - loss: -99921.6719 - val_loss: -99791.1953\n",
      "\n",
      "Epoch 00049: val_loss did not improve from -99829.14062\n",
      "Epoch 50/300\n",
      "175/175 [==============================] - 167s 952ms/step - loss: -99922.8359 - val_loss: -99806.5703\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -99829.14062\n",
      "Epoch 51/300\n",
      "175/175 [==============================] - 167s 953ms/step - loss: -99923.6797 - val_loss: -99776.7969\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -99829.14062\n",
      "Epoch 52/300\n",
      "175/175 [==============================] - 167s 952ms/step - loss: -99924.9375 - val_loss: -99779.0547\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -99829.14062\n",
      "Epoch 53/300\n",
      "175/175 [==============================] - 167s 952ms/step - loss: -99925.9531 - val_loss: -99769.4609\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -99829.14062\n",
      "Epoch 54/300\n",
      "175/175 [==============================] - 167s 951ms/step - loss: -99926.7344 - val_loss: -99791.2422\n",
      "\n",
      "Epoch 00054: val_loss did not improve from -99829.14062\n",
      "Epoch 55/300\n",
      "175/175 [==============================] - 167s 952ms/step - loss: -99927.8828 - val_loss: -99791.4062\n",
      "\n",
      "Epoch 00055: val_loss did not improve from -99829.14062\n",
      "Epoch 56/300\n",
      "175/175 [==============================] - 168s 958ms/step - loss: -99928.7734 - val_loss: -99772.9766\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00056: val_loss did not improve from -99829.14062\n",
      "Epoch 57/300\n",
      "175/175 [==============================] - 171s 973ms/step - loss: -99930.5625 - val_loss: -99771.2188\n",
      "\n",
      "Epoch 00057: val_loss did not improve from -99829.14062\n",
      "Epoch 58/300\n",
      "175/175 [==============================] - 168s 958ms/step - loss: -99930.9062 - val_loss: -99776.3672\n",
      "\n",
      "Epoch 00058: val_loss did not improve from -99829.14062\n",
      "Epoch 59/300\n",
      "175/175 [==============================] - 167s 950ms/step - loss: -99931.1172 - val_loss: -99769.9453\n",
      "\n",
      "Epoch 00059: val_loss did not improve from -99829.14062\n",
      "Epoch 60/300\n",
      "175/175 [==============================] - 167s 950ms/step - loss: -99931.2344 - val_loss: -99777.2969\n",
      "\n",
      "Epoch 00060: val_loss did not improve from -99829.14062\n",
      "Epoch 61/300\n",
      "175/175 [==============================] - 172s 979ms/step - loss: -99931.3359 - val_loss: -99771.4766\n",
      "\n",
      "Epoch 00061: val_loss did not improve from -99829.14062\n",
      "Epoch 62/300\n",
      "175/175 [==============================] - 170s 970ms/step - loss: -99931.4766 - val_loss: -99779.5234\n",
      "\n",
      "Epoch 00062: val_loss did not improve from -99829.14062\n",
      "Epoch 63/300\n",
      "175/175 [==============================] - 171s 978ms/step - loss: -99931.6328 - val_loss: -99769.3438\n",
      "\n",
      "Epoch 00063: val_loss did not improve from -99829.14062\n",
      "Epoch 64/300\n",
      "175/175 [==============================] - 185s 1s/step - loss: -99931.7344 - val_loss: -99781.9297\n",
      "\n",
      "Epoch 00064: val_loss did not improve from -99829.14062\n",
      "Epoch 65/300\n",
      "175/175 [==============================] - 181s 1s/step - loss: -99931.8594 - val_loss: -99775.0859\n",
      "\n",
      "Epoch 00065: val_loss did not improve from -99829.14062\n",
      "Epoch 66/300\n",
      "175/175 [==============================] - 166s 949ms/step - loss: -99931.9766 - val_loss: -99781.0312\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00066: val_loss did not improve from -99829.14062\n",
      "Epoch 00066: early stopping\n",
      "Training time:  11867.874955892563\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'Dice loss')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(history.history['val_loss'], label='Validation Dice Loss')\n",
    "plt.plot(history.history['loss'], label='Train Dice Loss')\n",
    "#plt.plot(batch_loss.losses)\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Dice loss')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 256, 256, 128, 1)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "filename = \"D3\"\n",
    "test_img, header = nrrd.read('./test_data/D3_resamp.nrrd')\n",
    "x, y, z = test_img.shape\n",
    "m_x = x//2\n",
    "m_y = y//2\n",
    "image = test_img[m_x-128:m_x+128,m_y-128:m_y+128,:]\n",
    "if filename[0] == 'D':\n",
    "    im_win = np.where(image<100,0, image)\n",
    "    im_win = np.where(image>700,0, im_win)\n",
    "elif filename[0] == 'K':\n",
    "    im_win = np.where(image<1100,0, image)\n",
    "    im_win = np.where(image>2000,0, im_win)\n",
    "else:\n",
    "    im_win = np.where(image<1100,0, image)\n",
    "    im_win = np.where(image>1350,0, im_win)\n",
    "\n",
    "image_norm = (im_win - np.min(im_win))/(np.max(im_win)-np.min(im_win))\n",
    "patch_shape =(256, 256, 128)\n",
    "image_pad = z_padding(image_norm)\n",
    "test_patches=patchify.patchify(image_pad, (256,256,128), step=128)\n",
    "pat_size = test_patches.shape\n",
    "test_patches = test_patches.squeeze()\n",
    "test_patches = np.expand_dims(test_patches, -1)\n",
    "pred_patches = np.empty(shape=test_patches.shape)\n",
    "pred_raw = np.empty(shape=test_patches.shape)\n",
    "print(test_patches.shape)\n",
    "for i in np.arange(test_patches.shape[0]):\n",
    "    pred = model.predict(np.expand_dims(test_patches[i,:,:,:,0],axis=0),batch_size=1, verbose=1)\n",
    "    pred_raw[i,:,:,:,0]=np.squeeze(pred)\n",
    "    pred = (pred > 0.005).astype(np.uint8)\n",
    "\n",
    "    pred = np.squeeze(pred)\n",
    "    pred_patches[i,:,:,:,0] =  pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 5, 256, 256, 128)\n"
     ]
    }
   ],
   "source": [
    "pred_patches=pred_patches.squeeze()\n",
    "pred_patches = np.expand_dims(np.expand_dims(pred_patches, axis=0),axis=0)\n",
    "pred_patches_r = pred_raw.squeeze()\n",
    "pred_patches_r = np.expand_dims(np.expand_dims(pred_patches_r, axis=0), axis=0)\n",
    "print(pred_patches.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reconstruct image from patches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "reconstructed = patchify.unpatchify(pred_patches,image_pad.shape)\n",
    "recon_raw = patchify.unpatchify(pred_patches_r, image_pad.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Morphological operations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "closed_once = ndimage.binary_closing(reconstructed, iterations=2).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Connected components"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "import cc3d\n",
    "import nrrd\n",
    "import numpy as np\n",
    "labels_out, N = cc3d.connected_components(closed_once, return_N=True)\n",
    "aorta_label = aorta_id(labels_out)\n",
    "labels_big = labels_out * (labels_out == aorta_label)\n",
    "nrrd.write('D3_win_unet_176.seg.nrrd', labels_big)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}